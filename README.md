# ğŸ™ï¸ Emotion Recognition from Audio using Deep Learning

This project detects emotions from audio clips (both **speech** and **song**) using the **RAVDESS** dataset. It includes:

-   A complete **data pipeline**
-   A **hybrid deep learning model** (Conv1D + LSTM)
-   A trained model saved in `.h5` format
-   A fully functional **Streamlit web app**
-   A testable Python script
-   Requirements and documentation for reproducibility

---

## ğŸ§  Project Description

This project builds a deep learning model to classify emotions in audio files using the **RAVDESS** dataset. Emotions detected:

-   Neutral
-   Calm
-   Happy
-   Sad
-   Angry
-   Fearful
-   Disgust
-   Surprised

The final model is deployed as a **Streamlit web app** where users can upload a `.wav` file and get the predicted emotion in real-time.

---

## âš™ï¸ Preprocessing Methodology

-   Loaded audio clips from both speech and song folders
-   Extracted emotion labels from filenames
-   Used `librosa` to extract **MFCCs** (40 features) per file
-   Applied **RandomOverSampler** to balance training data
-   Encoded labels using `LabelEncoder` and `to_categorical`
-   Reshaped MFCCs into 3D shape suitable for LSTM input

---

## ğŸ§© Model Pipeline

We used a **hybrid CNN + LSTM architecture**:

```text
Conv1D(128) â†’ BatchNorm â†’ MaxPool â†’ Dropout
Conv1D(256) â†’ BatchNorm â†’ MaxPool â†’ Dropout
Conv1D(256) â†’ BatchNorm â†’ MaxPool â†’ Dropout
LSTM(128) â†’ LSTM(256) â†’ Dropout â†’ LSTM(64)
Dense(8) with softmax


```
